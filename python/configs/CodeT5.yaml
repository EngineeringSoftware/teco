data:
  batch_size: 2
  val_batch_size: 16

model:
  pretrained_model: Salesforce/codet5-base

trainer:
  accelerator: gpu
  devices: -1
  strategy: fsdp
  precision: 16
  min_steps: 50
  max_epochs: 20
  accumulate_grad_batches: 4  # effective batch size 2*4(gpu)*4(accumulate) = 32

  callbacks:
    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        monitor: val/xmatch
        mode: max
        min_delta: 0.1
        patience: 3
        verbose: true
    # - class_path: pytorch_lightning.callbacks.StochasticWeightAveraging  # Incompatible with EarlyStopping
    - class_path: pytorch_lightning.callbacks.lr_monitor.LearningRateMonitor
      init_args:
        logging_interval: step

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.00005
    weight_decay: 0.01

lr_scheduler:
  class_path: torch.optim.lr_scheduler.OneCycleLR
  init_args:
    max_lr: 0.00005
    pct_start: 0.1
    div_factor: 1
    total_steps: 20
    anneal_strategy: linear

ckpt:
  monitor: val/xmatch
  mode: max
