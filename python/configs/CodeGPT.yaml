data:
  batch_size: 2
  val_batch_size: 4

model:
  pretrained_model: microsoft/CodeGPT-small-java-adaptedGPT2

trainer:
  auto_select_gpus: true
  gpus: -1
  strategy: fsdp
  precision: 16
  max_epochs: 20
  accumulate_grad_batches: 4  # effective batch size 1*4(gpu)*4(accumulate) = 32

  callbacks:
    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        monitor: val/acc
        mode: max
        min_delta: 0.1
        patience: 3
        verbose: true
    # - class_path: pytorch_lightning.callbacks.StochasticWeightAveraging  # Incompatible with EarlyStopping
    - class_path: pytorch_lightning.callbacks.lr_monitor.LearningRateMonitor
      init_args:
        logging_interval: step

optimizer:
  class_path: transformers.optimization.AdamW
  init_args:
    lr: 0.0001
    weight_decay: 0.01

lr_scheduler:
  class_path: torch.optim.lr_scheduler.OneCycleLR
  init_args:
    max_lr: 0.0001
    pct_start: 0.1
    div_factor: 1
    total_steps: 20
    anneal_strategy: linear

ckpt:
  monitor: val/acc
  mode: max
